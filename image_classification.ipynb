{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras # tf is a low model, every little detail, keras is high level api that they wrapped in tf2 \n",
    "from tensorflow.keras.preprocessing import image # this is dependant on a lib pip install pillow \n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.VGG16(weights='imagenet', include_top =True) # this means use the weights from when you trained the network on imagenet\n",
    "model.summary()\n",
    "feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n",
    "feat_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import random \n",
    "\n",
    "image_files = glob.glob('./Portraits/*.jpg') # this is because we have folders of folders so we need to enable it recursively\n",
    "\n",
    "# ** is a wildcard, anything that ends with .jpg, give me the name of it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = image_files[:670] # this grabs the first thousand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files # to show that its in random order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "features = []\n",
    "\n",
    "# for i, image_path in zip(range(0,len(image_files)), image_files) # same as below \n",
    "\n",
    "for i, image_path in enumerate(image_files): #it takes image_files and goes through and gives you each item and its associated index \n",
    "    if i % 10 == 0:\n",
    "        print(\"analyzed \" + str(i) + \" out of \" + str(len(image_files)))\n",
    "    \n",
    "    img = image.load_img(image_path, target_size=model.input_shape[1:3])\n",
    "    x = image.img_to_array(img) # turning it into a numpy array \n",
    "    #print(x)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    #print(x)\n",
    "    \n",
    "    feat = feat_extractor.predict(x)[0] # grabbing first value out of that prediction \n",
    "    #print(len(feat))\n",
    "    features.append(feat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans # this is sensitive to distance, so standardscaler normalizes it to distance \n",
    "\n",
    "ss = StandardScaler()\n",
    "scaled = ss.fit_transform(features)\n",
    "scaled[0] # looks like everything is btwn 1 and -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=20)\n",
    "clusters = kmeans.fit_predict(scaled)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "neighbors = NearestNeighbors(n_neighbors=5, metric='cosine').fit(scaled)\n",
    "_, closest = neighbors.kneighbors(kmeans.cluster_centers_)\n",
    "#cosine works better in high dimension space, we haven't done any dimentionality reduction \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest # index of all the images, 1st image, and the top 5 image by distance, list of a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ # this is the actual distance, its an _ when we're generally not using it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_width = 200\n",
    "\n",
    "grid_image = Image.new('RGB', (5 * img_width, len(closest) * 240), (0, 0, 0, 255))\n",
    "\n",
    "max_height = 0\n",
    "for cluster_row in closest:\n",
    "    max_height_temp = 0\n",
    "    xpos = 0\n",
    "    for col_num in cluster_row:\n",
    "        img = Image.open(image_files[col_num])\n",
    "        img_ar = img.width / img.height\n",
    "        img = img.resize((img_width, int(img_width / img_ar)), Image.ANTIALIAS)\n",
    "        max_height_temp = max(max_height_temp, img.height)\n",
    "        grid_image.paste(img, (xpos, max_height))\n",
    "        xpos = xpos + img.width\n",
    "    max_height = max_height + max_height_temp + 20 # adding 20 pixel margin\n",
    "\n",
    "plt.figure(figsize = (32,24))\n",
    "plt.imshow(grid_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimentionality reduction \n",
    "# we have a shit ton of features, and have high dimentionality, and we want to reduce it to a smaller dimention problem. ie going from 3d to 2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is data loss on dimentionality reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T-SNE it does a really good job of preserving spacial relationship\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "embedding = umap.UMAP().fit_transform(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(embedding)\n",
    "embedding_scaled = scaler.transform(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 4000\n",
    "height = 3000\n",
    "max_dim = 200\n",
    "\n",
    "full_image = Image.new('RGBA', (width, height), (0, 0, 0, 255))\n",
    "for img, pos in zip(image_files, embedding_scaled):\n",
    "    x = pos[0]\n",
    "    y = pos[1]\n",
    "    tile = Image.open(img)\n",
    "    rs = max(1, tile.width/max_dim, tile.height/max_dim)\n",
    "    tile = tile.resize((int(tile.width/rs), int(tile.height/rs)), Image.ANTIALIAS)\n",
    "    full_image.paste(tile, (int((width-max_dim)*x), int((height-max_dim)*y)), mask=tile.convert('RGBA'))\n",
    "\n",
    "plt.figure(figsize = (32,24))\n",
    "plt.imshow(full_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterfairy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 23\n",
    "ny = 25\n",
    "\n",
    "grid_assignment = rasterfairy.transformPointCloud2D(embedding, target=(nx, ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_width = 100\n",
    "tile_height = 100\n",
    "\n",
    "full_width = tile_width * nx\n",
    "full_height = tile_height * ny\n",
    "aspect_ratio = tile_width / tile_height\n",
    "\n",
    "grid_image = Image.new('RGB', (full_width, full_height), (0, 0, 0, 255))\n",
    "\n",
    "for img, grid_pos in zip(image_files, grid_assignment[0]):\n",
    "    idx_x, idx_y = grid_pos\n",
    "    x, y = tile_width * idx_x, tile_height * idx_y\n",
    "    tile = Image.open(img)\n",
    "    tile_ar = tile.width / tile.height\n",
    "    tile = tile.resize((int(0.8 * tile_width), int(0.8 * tile_height / tile_ar)), Image.ANTIALIAS)\n",
    "    grid_image.paste(tile, (int(x), int(y)))\n",
    "\n",
    "plt.figure(figsize = (16,12))\n",
    "plt.imshow(grid_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = []\n",
    "\n",
    "for image, grid_pos, cluster_pos in zip(image_files, grid_assignment[0], embedding_scale):\n",
    "    lookup.append({\n",
    "        'filename': image_files.replace('./', \"\"),\n",
    "        'grid_post': grid_pos.tolist(),\n",
    "        'cluster_pos': cluster_pos.tolist()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('image_umap_position.json', 'w') as outfile\"\n",
    "    json.dump(lookup, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
